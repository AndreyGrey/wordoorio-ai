#!/usr/bin/env python3
"""
üîç DEDUPLICATION SERVICE - –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —Å—Ö–æ–∂–∏—Ö —Ö–∞–π–ª–∞–π—Ç–æ–≤
—Å —É—á–µ—Ç–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤.
"""

import re
import difflib
from typing import List, Dict, Set, Tuple
from dataclasses import dataclass
from enum import Enum
from contracts.analysis_contracts import Highlight

class SimilarityType(Enum):
    """–¢–∏–ø—ã —Å—Ö–æ–∂–µ—Å—Ç–∏"""
    EXACT_DUPLICATE = "exact_duplicate"
    MORPHOLOGICAL = "morphological"  # walk/walking/walked
    SEMANTIC = "semantic"           # big/large/huge
    PARTIAL_OVERLAP = "partial_overlap"  # "make decision" vs "decision making"

@dataclass
class DuplicationInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–∏"""
    original_index: int
    duplicate_index: int
    similarity_score: float
    similarity_type: SimilarityType
    explanation: str

class DeduplicationService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ö–∞–π–ª–∞–π—Ç–æ–≤"""
    
    def __init__(self):
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        self.similarity_threshold = 0.8
        self.partial_overlap_threshold = 0.6
        self.morphological_threshold = 0.85
        
        # –ö—ç—à –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self._similarity_cache: Dict[Tuple[str, str], float] = {}
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        self.morphological_patterns = [
            (r'(\w+)ing$', r'\1'),      # walking -> walk
            (r'(\w+)ed$', r'\1'),       # walked -> walk
            (r'(\w+)s$', r'\1'),        # makes -> make
            (r'(\w+)er$', r'\1'),       # bigger -> big
            (r'(\w+)est$', r'\1'),      # biggest -> big
            (r'(\w+)ly$', r'\1'),       # quickly -> quick
        ]
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å—Ö–æ–∂–∏–µ –≥—Ä—É–ø–ø—ã —Å–ª–æ–≤
        self.semantic_groups = {
            'big': ['large', 'huge', 'massive', 'enormous', 'giant', 'vast'],
            'small': ['tiny', 'little', 'minimal', 'minor', 'compact'],
            'good': ['excellent', 'great', 'wonderful', 'fantastic', 'amazing'],
            'bad': ['terrible', 'awful', 'horrible', 'poor', 'dreadful'],
            'fast': ['quick', 'rapid', 'swift', 'speedy', 'hasty'],
            'slow': ['sluggish', 'gradual', 'delayed', 'leisurely'],
            'smart': ['intelligent', 'clever', 'brilliant', 'wise', 'bright'],
            'important': ['crucial', 'vital', 'essential', 'critical', 'significant'],
            'easy': ['simple', 'effortless', 'straightforward', 'basic'],
            'difficult': ['hard', 'challenging', 'complex', 'tough', 'demanding']
        }
        
    def deduplicate_highlights(self, highlights: List[Highlight]) -> Tuple[List[Highlight], List[DuplicationInfo]]:
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ —Ö–∞–π–ª–∞–π—Ç–æ–≤
        
        Returns:
            Tuple[List[Highlight], List[DuplicationInfo]]: 
                –û—á–∏—â–µ–Ω–Ω—ã–µ —Ö–∞–π–ª–∞–π—Ç—ã –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
        """
        if not highlights:
            return [], []
        
        print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é {len(highlights)} —Ö–∞–π–ª–∞–π—Ç–æ–≤...", flush=True)
        
        duplications = []
        indices_to_remove = set()
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ö–∞–π–ª–∞–π—Ç —Å –∫–∞–∂–¥—ã–º
        for i in range(len(highlights)):
            if i in indices_to_remove:
                continue
                
            for j in range(i + 1, len(highlights)):
                if j in indices_to_remove:
                    continue
                
                dup_info = self._check_similarity(highlights[i], highlights[j], i, j)
                
                if dup_info:
                    duplications.append(dup_info)
                    # –£–¥–∞–ª—è–µ–º —Ö–∞–π–ª–∞–π—Ç —Å –º–µ–Ω—å—à–∏–º importance_score
                    if highlights[i].importance_score >= highlights[j].importance_score:
                        indices_to_remove.add(j)
                        print(f"üóëÔ∏è  –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç: '{highlights[j].highlight}' (—Å—Ö–æ–∂ —Å '{highlights[i].highlight}')", flush=True)
                    else:
                        indices_to_remove.add(i)
                        print(f"üóëÔ∏è  –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç: '{highlights[i].highlight}' (—Å—Ö–æ–∂ —Å '{highlights[j].highlight}')", flush=True)
                        break  # –í—ã—Ö–æ–¥–∏–º –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ü–∏–∫–ª–∞, —Ç–∞–∫ –∫–∞–∫ i —É–∂–µ –ø–æ–º–µ—á–µ–Ω –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        clean_highlights = [
            highlights[i] for i in range(len(highlights)) 
            if i not in indices_to_remove
        ]
        
        print(f"‚úÖ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(highlights)} -> {len(clean_highlights)} —Ö–∞–π–ª–∞–π—Ç–æ–≤", flush=True)
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ –∏ —É–¥–∞–ª–µ–Ω–æ {len(indices_to_remove)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤", flush=True)
        
        return clean_highlights, duplications
    
    def _check_similarity(self, h1: Highlight, h2: Highlight, idx1: int, idx2: int) -> DuplicationInfo:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ö–∞–π–ª–∞–π—Ç–æ–≤"""
        text1 = h1.highlight.lower().strip()
        text2 = h2.highlight.lower().strip()
        
        # –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        if text1 == text2:
            return DuplicationInfo(
                original_index=idx1,
                duplicate_index=idx2,
                similarity_score=1.0,
                similarity_type=SimilarityType.EXACT_DUPLICATE,
                explanation=f"–¢–æ—á–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç: '{text1}'"
            )
        
        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        morph_score = self._calculate_morphological_similarity(text1, text2)
        if morph_score >= self.morphological_threshold:
            return DuplicationInfo(
                original_index=idx1,
                duplicate_index=idx2,
                similarity_score=morph_score,
                similarity_type=SimilarityType.MORPHOLOGICAL,
                explanation=f"–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: '{text1}' –∏ '{text2}'"
            )
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
        semantic_score = self._calculate_semantic_similarity(text1, text2)
        if semantic_score >= self.similarity_threshold:
            return DuplicationInfo(
                original_index=idx1,
                duplicate_index=idx2,
                similarity_score=semantic_score,
                similarity_type=SimilarityType.SEMANTIC,
                explanation=f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å—Ö–æ–∂–∏–µ: '{text1}' –∏ '{text2}'"
            )
        
        # –ß–∞—Å—Ç–∏—á–Ω–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–ª—è —Ñ—Ä–∞–∑
        if ' ' in text1 or ' ' in text2:
            overlap_score = self._calculate_partial_overlap(text1, text2)
            if overlap_score >= self.partial_overlap_threshold:
                return DuplicationInfo(
                    original_index=idx1,
                    duplicate_index=idx2,
                    similarity_score=overlap_score,
                    similarity_type=SimilarityType.PARTIAL_OVERLAP,
                    explanation=f"–ß–∞—Å—Ç–∏—á–Ω–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: '{text1}' –∏ '{text2}'"
                )
        
        return None
    
    def _calculate_morphological_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = (text1, text2) if text1 < text2 else (text2, text1)
        if cache_key in self._similarity_cache:
            return self._similarity_cache[cache_key]
        
        # –£–±–∏—Ä–∞–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        stem1 = self._get_word_stem(text1)
        stem2 = self._get_word_stem(text2)
        
        # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç - –≤—ã—Å–æ–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
        if stem1 == stem2 and len(stem1) > 3:
            score = 0.95
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º difflib –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            score = difflib.SequenceMatcher(None, stem1, stem2).ratio()
        
        self._similarity_cache[cache_key] = score
        return score
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å"""
        # –ò—â–µ–º –≤ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø–∞—Ö
        for group_words in self.semantic_groups.values():
            if text1 in group_words and text2 in group_words:
                return 0.9
        
        # –î–ª—è —Ñ—Ä–∞–∑ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if ' ' in text1 and ' ' in text2:
            words1 = set(text1.split())
            words2 = set(text2.split())
            
            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
            stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
            words1 = words1 - stop_words
            words2 = words2 - stop_words
            
            if words1 and words2:
                intersection = len(words1 & words2)
                union = len(words1 | words2)
                return intersection / union if union > 0 else 0
        
        # –ë–∞–∑–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫
        return difflib.SequenceMatcher(None, text1, text2).ratio()
    
    def _calculate_partial_overlap(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–ª—è —Ñ—Ä–∞–∑"""
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–¥–Ω–∞ —Ñ—Ä–∞–∑–∞ –¥—Ä—É–≥—É—é
        if words1.issubset(words2) or words2.issubset(words1):
            return 0.8
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        if union == 0:
            return 0
        
        jaccard_score = intersection / union
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤
        if self._check_word_order_similarity(text1, text2):
            jaccard_score += 0.1
        
        return min(jaccard_score, 1.0)
    
    def _get_word_stem(self, word: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Å–Ω–æ–≤—É —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è"""
        for pattern, replacement in self.morphological_patterns:
            match = re.match(pattern, word)
            if match:
                return match.group(1)
        return word
    
    def _check_word_order_similarity(self, text1: str, text2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ—Ä—è–¥–∫–∞ —Å–ª–æ–≤ –≤ —Ñ—Ä–∞–∑–∞—Ö"""
        words1 = text1.split()
        words2 = text2.split()
        
        # –ò—â–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –≤ —Å—Ö–æ–∂–∏—Ö –ø–æ–∑–∏—Ü–∏—è—Ö
        common_positions = 0
        for i, word1 in enumerate(words1):
            for j, word2 in enumerate(words2):
                if word1 == word2 and abs(i - j) <= 1:  # –°–ª–æ–≤–æ –≤ —Ç–æ–π –∂–µ –∏–ª–∏ —Å–æ—Å–µ–¥–Ω–µ–π –ø–æ–∑–∏—Ü–∏–∏
                    common_positions += 1
                    break
        
        return common_positions >= min(len(words1), len(words2)) * 0.5
    
    def analyze_duplications(self, duplications: List[DuplicationInfo]) -> Dict[str, any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã"""
        if not duplications:
            return {
                'total_duplicates': 0,
                'by_type': {},
                'average_similarity': 0,
                'recommendations': []
            }
        
        by_type = {}
        total_similarity = 0
        
        for dup in duplications:
            dup_type = dup.similarity_type.value
            if dup_type not in by_type:
                by_type[dup_type] = 0
            by_type[dup_type] += 1
            total_similarity += dup.similarity_score
        
        average_similarity = total_similarity / len(duplications)
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        if by_type.get('exact_duplicate', 0) > 0:
            recommendations.append("–ù–∞–π–¥–µ–Ω—ã —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏–∫—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è")
        if by_type.get('morphological', 0) > 3:
            recommendations.append("–ú–Ω–æ–≥–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Å—Ç–µ–º–º–∏–Ω–≥")
        if by_type.get('semantic', 0) > 2:
            recommendations.append("–ù–∞–π–¥–µ–Ω—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ - –≤–æ–∑–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç")
        
        return {
            'total_duplicates': len(duplications),
            'by_type': by_type,
            'average_similarity': round(average_similarity, 3),
            'recommendations': recommendations
        }
    
    def get_statistics(self, original_count: int, final_count: int, duplications: List[DuplicationInfo]) -> Dict[str, any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        removed_count = original_count - final_count
        removal_percentage = (removed_count / original_count * 100) if original_count > 0 else 0
        
        return {
            'original_count': original_count,
            'final_count': final_count,
            'removed_count': removed_count,
            'removal_percentage': round(removal_percentage, 1),
            'duplications_found': len(duplications),
            'quality_improvement': self._estimate_quality_improvement(duplications)
        }
    
    def _estimate_quality_improvement(self, duplications: List[DuplicationInfo]) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞"""
        if not duplications:
            return "no_change"
        
        high_similarity_count = sum(1 for dup in duplications if dup.similarity_score > 0.9)
        
        if high_similarity_count >= len(duplications) * 0.7:
            return "significant"
        elif high_similarity_count >= len(duplications) * 0.4:
            return "moderate"
        else:
            return "minor"

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
_deduplication_service = None

def get_deduplication_service() -> DeduplicationService:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä DeduplicationService (Singleton)"""
    global _deduplication_service
    if _deduplication_service is None:
        _deduplication_service = DeduplicationService()
    return _deduplication_service