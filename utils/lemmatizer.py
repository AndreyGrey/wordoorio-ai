#!/usr/bin/env python3
"""
–õ–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (spaCy)
"""

import spacy

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
_nlp = None


def _get_nlp():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏"""
    global _nlp
    if _nlp is None:
        print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ...", flush=True)
        _nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞", flush=True)
    return _nlp


def lemmatize(text: str) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É –≤ —Å–ª–æ–≤–∞—Ä–Ω—É—é —Ñ–æ—Ä–º—É

    –ü—Ä–∏–º–µ—Ä—ã:
        - "incentives" ‚Üí "incentive"
        - "running" ‚Üí "run"
        - "went" ‚Üí "go"
        - "gave up" ‚Üí "give up"
        - "making sense" ‚Üí "make sense"

    Args:
        text: –°–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑–∞ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏

    Returns:
        –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text or not text.strip():
        return text

    nlp = _get_nlp()
    doc = nlp(text.strip())

    # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
    lemmas = [token.lemma_ for token in doc]

    return " ".join(lemmas)


# –¢–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
if __name__ == "__main__":
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä...\n")

    english_tests = [
        "incentives",
        "running",
        "went",
        "bigger",
        "stories",
        "gave up",
        "making sense",
        "came across",
        "compelling arguments",
        "amplifying",
    ]

    for test in english_tests:
        result = lemmatize(test)
        print(f"  '{test}' ‚Üí '{result}'")

    print("\n‚úÖ –¢–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
