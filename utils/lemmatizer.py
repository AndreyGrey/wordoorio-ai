#!/usr/bin/env python3
"""
–õ–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤

- –ê–Ω–≥–ª–∏–π—Å–∫–∏–π: spaCy (en_core_web_sm)
- –†—É—Å—Å–∫–∏–π: pymorphy2
"""

import spacy

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
_nlp = None
_morph = None


def _get_nlp():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏"""
    global _nlp
    if _nlp is None:
        print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ...", flush=True)
        _nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞", flush=True)
    return _nlp


def lemmatize_with_pos(text: str) -> tuple:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É –≤ —Å–ª–æ–≤–∞—Ä–Ω—É—é —Ñ–æ—Ä–º—É + –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–ª–∞–≥ –ø—Ä–∏—á–∞—Å—Ç–∏—è.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è EN ‚Üî RU: –µ—Å–ª–∏ EN = –ø—Ä–∏—á–∞—Å—Ç–∏–µ,
    —Ç–æ RU –ø–µ—Ä–µ–≤–æ–¥ —Ç–æ–∂–µ –Ω–µ –Ω—É–∂–Ω–æ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å.

    Args:
        text: –°–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑–∞ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏

    Returns:
        tuple: (–ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, is_participle)
            - is_participle = True –µ—Å–ª–∏ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ ‚Äî –ø—Ä–∏—á–∞—Å—Ç–∏–µ (VBN/VBG)
    """
    if not text or not text.strip():
        return text, False

    nlp = _get_nlp()
    doc = nlp(text.strip())

    lemmas = []
    is_participle = False

    for i, token in enumerate(doc):
        if token.tag_ in ('VBN', 'VBG'):
            # VBN = past participle (embroiled, broken, written)
            # VBG = gerund/present participle (amplifying, running)
            lemmas.append(token.text.lower())
            if i == 0:  # –ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ ‚Äî –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –≤—Å–µ–≥–æ highlight
                is_participle = True
        else:
            lemmas.append(token.lemma_)

    return " ".join(lemmas), is_participle


def lemmatize(text: str) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É –≤ —Å–ª–æ–≤–∞—Ä–Ω—É—é —Ñ–æ—Ä–º—É

    –ü—Ä–∏–º–µ—Ä—ã:
        - "incentives" ‚Üí "incentive"
        - "went" ‚Üí "go"
        - "gave up" ‚Üí "give up"
        - "embroiled" ‚Üí "embroiled" (–ø—Ä–∏—á–∞—Å—Ç–∏–µ ‚Äî –Ω–µ —Ç—Ä–æ–≥–∞–µ–º)
        - "amplifying" ‚Üí "amplifying" (–≥–µ—Ä—É–Ω–¥–∏–π ‚Äî –Ω–µ —Ç—Ä–æ–≥–∞–µ–º)

    Args:
        text: –°–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑–∞ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏

    Returns:
        –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    lemma, _ = lemmatize_with_pos(text)
    return lemma


def _get_morph():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ pymorphy2 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
    global _morph
    if _morph is None:
        import pymorphy2
        print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º pymorphy2 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ...", flush=True)
        _morph = pymorphy2.MorphAnalyzer()
        print("‚úÖ pymorphy2 –∑–∞–≥—Ä—É–∂–µ–Ω", flush=True)
    return _morph


def lemmatize_russian(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –≤ —Å–ª–æ–≤–∞—Ä–Ω—É—é —Ñ–æ—Ä–º—É.

    –î–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Å–ª–æ–≤:
    - –ì–ª–∞–≥–æ–ª—ã ‚Üí –∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤ (normal_form)
    - –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ/–ø—Ä–∏—á–∞—Å—Ç–∏—è ‚Üí –º.—Ä. –µ–¥.—á. –∏–º.–ø. (inflect, –ù–ï normal_form!)
    - –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ ‚Üí –µ–¥.—á. –∏–º.–ø.

    –î–ª—è —Ñ—Ä–∞–∑:
    - –ù–ï —Ç—Ä–æ–≥–∞–µ–º ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å –æ—Ç AI-–∞–≥–µ–Ω—Ç–∞
    - –ü—Ä–æ–º–ø—Ç v2.0 —É–∂–µ –ø—Ä–æ—Å–∏—Ç —Å–ª–æ–≤–∞—Ä–Ω—É—é —Ñ–æ—Ä–º—É
    - AI –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ–º –≤–Ω—É—Ç—Ä–∏ —Ñ—Ä–∞–∑—ã

    –ü—Ä–∏–º–µ—Ä—ã:
    - "–∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ" ‚Üí "–∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π" (–ø—Ä–∏—á–∞—Å—Ç–∏–µ ‚Üí –º.—Ä. –µ–¥.—á.)
    - "–Ω–∞—Ä—É—à–∞—é—â–∞—è" ‚Üí "–Ω–∞—Ä—É—à–∞—é—â–∏–π" (–ø—Ä–∏—á–∞—Å—Ç–∏–µ ‚Üí –º.—Ä. –µ–¥.—á., –ù–ï "–Ω–∞—Ä—É—à–∞—Ç—å"!)
    - "–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å" ‚Üí "–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è" (–≥–ª–∞–≥–æ–ª ‚Üí –∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤)
    - "–∑–∞–∫—Ä—ã—Ç–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞" ‚Üí "–∑–∞–∫—Ä—ã—Ç–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞" (—Ñ—Ä–∞–∑–∞ ‚Äî –Ω–µ —Ç—Ä–æ–≥–∞–µ–º)

    Args:
        text: –°–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text or not text.strip():
        return text

    morph = _get_morph()
    words = text.strip().split()

    if len(words) == 1:
        # –û–¥–∏–Ω–æ—á–Ω–æ–µ —Å–ª–æ–≤–æ ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å —É—á—ë—Ç–æ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏
        parsed = morph.parse(words[0])[0]
        pos = parsed.tag.POS

        if pos == 'VERB' or pos == 'INFN':
            # –ì–ª–∞–≥–æ–ª ‚Üí –∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤
            return parsed.normal_form
        elif pos in ('ADJF', 'ADJS', 'PRTF', 'PRTS'):
            # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ –∏–ª–∏ –ø—Ä–∏—á–∞—Å—Ç–∏–µ ‚Üí –º.—Ä. –µ–¥.—á. –∏–º.–ø.
            # –í–ê–ñ–ù–û: –ù–ï normal_form, –∏–Ω–∞—á–µ –ø—Ä–∏—á–∞—Å—Ç–∏–µ —Å—Ç–∞–Ω–µ—Ç –≥–ª–∞–≥–æ–ª–æ–º!
            inflected = parsed.inflect({'masc', 'sing', 'nomn'})
            if inflected:
                return inflected.word
            # Fallback –µ—Å–ª–∏ inflect –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
            return parsed.normal_form
        elif pos == 'NOUN':
            # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ ‚Üí –µ–¥.—á. –∏–º.–ø.
            inflected = parsed.inflect({'sing', 'nomn'})
            if inflected:
                return inflected.word
            return parsed.normal_form
        else:
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏ ‚Äî normal_form
            return parsed.normal_form
    else:
        # –§—Ä–∞–∑–∞ (2+ —Å–ª–æ–≤) ‚Äî –ù–ï —Ç—Ä–æ–≥–∞–µ–º, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
        # AI-–∞–≥–µ–Ω—Ç —Å –ø—Ä–æ–º–ø—Ç–æ–º v2.0 —É–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä–Ω—É—é —Ñ–æ—Ä–º—É
        # –∏ –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ–º –≤–Ω—É—Ç—Ä–∏ —Ñ—Ä–∞–∑—ã
        return text.strip()


# –¢–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
if __name__ == "__main__":
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä...\n")

    print("=== –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (spaCy) ‚Äî lemmatize_with_pos() ===")
    english_tests = [
        # (–≤—Ö–æ–¥, –æ–∂–∏–¥–∞–µ–º–∞—è_–ª–µ–º–º–∞, –æ–∂–∏–¥–∞–µ–º—ã–π_is_participle)
        # –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ ‚Üí –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É—é—Ç—Å—è, is_participle=False
        ("incentives", "incentive", False),
        ("went", "go", False),
        ("bigger", "big", False),
        ("stories", "story", False),
        # –ü—Ä–∏—á–∞—Å—Ç–∏—è (VBN, VBG) ‚Üí –ù–ï –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É—é—Ç—Å—è, is_participle=True
        ("embroiled", "embroiled", True),   # VBN
        ("amplifying", "amplifying", True),  # VBG
        ("running", "running", True),        # VBG
        ("broken", "broken", True),          # VBN
        ("peppered", "peppered", True),      # VBN
        # –§—Ä–∞–∑—ã ‚Äî is_participle –ø–æ –ø–µ—Ä–≤–æ–º—É —Å–ª–æ–≤—É
        ("gave up", "give up", False),       # gave=VBD, –Ω–µ –ø—Ä–∏—á–∞—Å—Ç–∏–µ
        ("came across", "come across", False),
        ("peppered with", "peppered with", True),  # peppered=VBN
    ]

    for test, expected_lemma, expected_participle in english_tests:
        lemma, is_part = lemmatize_with_pos(test)
        lemma_ok = lemma == expected_lemma
        part_ok = is_part == expected_participle
        if lemma_ok and part_ok:
            status = "‚úì"
        else:
            status = f"‚úó (–ª–µ–º–º–∞: '{lemma}', participle: {is_part})"
        print(f"  '{test}' ‚Üí '{lemma}' [participle={is_part}] {status}")

    print("\n=== –†—É—Å—Å–∫–∏–π (pymorphy2) ===")
    russian_tests = [
        # –ü—Ä–∏—á–∞—Å—Ç–∏—è/–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ ‚Üí –º.—Ä. –µ–¥.—á. (–ù–ï –∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤!)
        ("–∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ", "–∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π"),
        ("–Ω–∞—Ä—É—à–∞—é—â–∞—è", "–Ω–∞—Ä—É—à–∞—é—â–∏–π"),  # –ù–ï "–Ω–∞—Ä—É—à–∞—Ç—å"!
        ("–ø–æ–¥–¥–µ–ª—å–Ω—ã–µ", "–ø–æ–¥–¥–µ–ª—å–Ω—ã–π"),
        ("–ø–æ–¥–¥–µ–ª—å–Ω–∞—è", "–ø–æ–¥–¥–µ–ª—å–Ω—ã–π"),
        ("—Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—ë–Ω–Ω–æ–µ", "—Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—ë–Ω–Ω—ã–π"),
        # –ì–ª–∞–≥–æ–ª—ã ‚Üí –∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤
        ("–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å", "–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è"),
        ("–∫–∞—Å–∞–µ—Ç—Å—è", "–∫–∞—Å–∞—Ç—å—Å—è"),
        ("–ø–æ—è–≤–∏–ª—Å—è", "–ø–æ—è–≤–∏—Ç—å—Å—è"),
        # –§—Ä–∞–∑—ã ‚Üí –Ω–µ —Ç—Ä–æ–≥–∞–µ–º (AI —É–∂–µ –≤–µ—Ä–Ω—É–ª –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É)
        ("–∑–∞–∫—Ä—ã—Ç–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞", "–∑–∞–∫—Ä—ã—Ç–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞"),
        ("–∑–∞—Å—ã–ø–∞–ª–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏", "–∑–∞—Å—ã–ø–∞–ª–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏"),
        ("–≤—Ç—è–Ω—É—Ç –≤ –∫–æ–Ω—Ñ–ª–∏–∫—Ç", "–≤—Ç—è–Ω—É—Ç –≤ –∫–æ–Ω—Ñ–ª–∏–∫—Ç"),
    ]

    for test, expected in russian_tests:
        result = lemmatize_russian(test)
        status = "‚úì" if result == expected else f"‚úó (–æ–∂–∏–¥–∞–ª–æ—Å—å '{expected}')"
        print(f"  '{test}' ‚Üí '{result}' {status}")

    print("\n‚úÖ –¢–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
