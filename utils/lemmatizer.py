#!/usr/bin/env python3
"""
üî§ –õ–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä
–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã –≤ —Å–ª–æ–≤–∞—Ä–Ω—É—é —Ñ–æ—Ä–º—É (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏ —Ä—É—Å—Å–∫–∏–π)
"""

import spacy
import pymorphy2

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
_nlp = None
_morph = None

def _get_nlp():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏"""
    global _nlp
    if _nlp is None:
        print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ...", flush=True)
        _nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞", flush=True)
    return _nlp

def _get_morph():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ –º–æ—Ä—Ñ–æ–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    global _morph
    if _morph is None:
        print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º pymorphy2 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ...", flush=True)
        _morph = pymorphy2.MorphAnalyzer()
        print("‚úÖ –ú–æ—Ä—Ñ–æ–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω", flush=True)
    return _morph


def lemmatize(text: str) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É –≤ —Å–ª–æ–≤–∞—Ä–Ω—É—é —Ñ–æ—Ä–º—É

    –ü—Ä–∏–º–µ—Ä—ã:
        - "incentives" ‚Üí "incentive"
        - "running" ‚Üí "run"
        - "went" ‚Üí "go"
        - "gave up" ‚Üí "give up"
        - "making sense" ‚Üí "make sense"

    Args:
        text: –°–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑–∞ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏

    Returns:
        –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text or not text.strip():
        return text

    nlp = _get_nlp()
    doc = nlp(text.strip())

    # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
    lemmas = [token.lemma_ for token in doc]

    return " ".join(lemmas)


def lemmatize_batch(texts: list[str]) -> list[str]:
    """
    –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ (–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —á–µ–º –ø–æ –æ–¥–Ω–æ–º—É)

    Args:
        texts: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∏–ª–∏ —Ñ—Ä–∞–∑

    Returns:
        –°–ø–∏—Å–æ–∫ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    """
    if not texts:
        return []

    nlp = _get_nlp()

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥
    results = []
    for doc in nlp.pipe(texts):
        lemmas = [token.lemma_ for token in doc]
        results.append(" ".join(lemmas))

    return results


def lemmatize_russian(text: str) -> str:
    """
    –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç (—Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É)

    –ü—Ä–∏–º–µ—Ä—ã:
        - "—Å—Ç–∏–º—É–ª—ã" ‚Üí "—Å—Ç–∏–º—É–ª"
        - "–±–µ–≥—É—â–∏–π" ‚Üí "–±–µ–∂–∞—Ç—å"
        - "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å–æ–º" ‚Üí "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å"

    Args:
        text: –†—É—Å—Å–∫–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑–∞

    Returns:
        –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text or not text.strip():
        return text

    morph = _get_morph()

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    words = text.strip().split()
    lemmas = []

    for word in words:
        # –ü–∞—Ä—Å–∏–º —Å–ª–æ–≤–æ –∏ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π (–Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π) –≤–∞—Ä–∏–∞–Ω—Ç
        parsed = morph.parse(word)
        if parsed:
            lemma = parsed[0].normal_form
            lemmas.append(lemma)
        else:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            lemmas.append(word)

    return " ".join(lemmas)


# –¢–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
if __name__ == "__main__":
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä...\n")

    print("üìù –ê–Ω–≥–ª–∏–π—Å–∫–∏–π:")
    english_tests = [
        "incentives",
        "running",
        "went",
        "bigger",
        "stories",
        "gave up",
        "making sense",
        "came across",
        "compelling arguments",
    ]

    for test in english_tests:
        result = lemmatize(test)
        print(f"  '{test}' ‚Üí '{result}'")

    print("\nüìù –†—É—Å—Å–∫–∏–π:")
    russian_tests = [
        "—Å—Ç–∏–º—É–ª—ã",
        "—Å—Ç–∏–º—É–ª–∏—Ä—É—é—â–∏–π",
        "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å–æ–º",
        "–±–µ–≥—É—â–∏–π",
        "—Å–¥–∞–ª–∏—Å—å",
    ]

    for test in russian_tests:
        result = lemmatize_russian(test)
        print(f"  '{test}' ‚Üí '{result}'")

    print("\n‚úÖ –¢–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
